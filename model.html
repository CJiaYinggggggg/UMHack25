<!-- models.html -->
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Models Overview</title>
  <link rel="stylesheet" href="style.css">
</head>
<body>
  <header>
    <h1>Models</h1>
    <nav><a href="index.html">&#8592; Home</a></nav>
  </header>
  
  <section>
    <h2>Hidden Markov Model (HMM)</h2>
<p>An HMM uses latent (hidden) states to model market regimes. It consists of:</p>
<ul>
  <li><strong>States:</strong> e.g., Bull, Bear.</li>
  <li><strong>Transition Matrix:</strong> Probabilities of switching between states.</li>
  <li><strong>Emission Probabilities:</strong> Likelihood of observations given a state.</li>
</ul>

<h3>Data Collection and Preparation</h3>
<p>You're using a combined dataset that includes:</p>
<ul>
  <li><strong>On-chain cryptocurrency data:</strong> blockchain metrics like transactions, volume, etc.</li>
  <li><strong>Macroeconomic indicators:</strong> inflation rates, interest rates, and other economic factors.</li>
</ul>

<h3>Feature Engineering</h3>
<p>Several technical indicators and metrics were created to analyze cryptocurrency price movements:</p>
<ul>
  <li><strong>Returns and Log Returns:</strong> Measure price changes between periods.</li>
  <li><strong>Volatility:</strong> Measures price fluctuation/dispersion.</li>
  <li><strong>Moving Averages (MA10):</strong> 10-period moving average to smooth price trends.</li>
  <li><strong>Rate of Change (ROC_5, ROC_10, ROC_20):</strong> Measure price momentum over 5, 10, and 20 periods.</li>
  <li><strong>Volume Change:</strong> Changes in trading volume.</li>
  <li><strong>Fractional Price Relationships:</strong>
    <ul>
      <li>Frac_Open_Close: Relationship between opening and closing prices</li>
      <li>Frac_Open_High: Relationship between opening and highest prices</li>
      <li>Frac_Open_Low: Relationship between opening and lowest prices</li>
      <li>Frac_High_Low: Range of price movement (volatility indicator)</li>
    </ul>
  </li>
  <li><strong>ATR_14:</strong> Average True Range over 14 periods (volatility indicator).</li>
  <li><strong>RSI_14:</strong> Relative Strength Index over 14 periods (momentum indicator).</li>
</ul>

<h3>Feature Selection</h3>
<p>Using correlation analysis, a smaller subset of the most informative, low‑collinearity features was selected:</p>
<ul>
  <li><strong>Frac_High_Low</strong> (price volatility/range)</li>
  <li><strong>ATR_14</strong> (volatility)</li>
  <li><strong>Returns</strong> (price changes)</li>
  <li><strong>macro_index_kernel</strong> (macroeconomic factor composite)</li>
</ul>

<h3>Model Training</h3>
<p>A systematic approach was used to train the HMM:</p>
<ul>
  <li>Split data into training, testing, and validation sets.</li>
  <li>Use Grid Search to find optimal hyperparameters.</li>
</ul>

<h3>Regime Identification</h3>
<p>Regime detection is employed to identify different market states, such as bull, bear, or sideways markets. This allows:</p>
<ul>
  <li>Recognition that financial markets behave differently under varying conditions.</li>
  <li>Training of regime‑specific strategies or models for improved performance.</li>
</ul>

<h3>Datetime Segmentation</h3>
<p>Once regimes are identified, data is segmented by the corresponding time periods to:</p>
<ul>
  <li>Analyze behavior differences across regimes.</li>
  <li>Potentially train and backtest strategies that adapt to the current market regime.</li>
</ul>

  </section>
  <section>
    <h2>XGBoost</h2>
    <p>XGBoost is a gradient boosting framework that uses decision trees. Key features include:</p>
    <ul>
      <li><strong>Boosting:</strong> Sequentially adds weak learners to minimize loss.</li>
      <li><strong>Regularization:</strong> Controls overfitting with L1 and L2 penalties.</li>
      <li><strong>Parallelization:</strong> Efficiently handles large datasets.</li>
    <h3>Step 1</h3>
    <p>Obtain dataset from different regimes (regimes_0.csv, regimes_1.csv, regimes_2.csv..)</p>
    <h3>Step 2</h3>
    <p>Drop column that are not important (compute using HMM calculation)
    </p>
    <h3>Step 3</h3>
    <p><strong>Creating Rolling Windows (create_window_sequences)</strong>
    </p>
    
    <p>Purpose: Converts time series data into supervised learning format by creating rolling windows of past features and corresponding targets.</p>
    <pre><code>   
    def create_window_sequences(self, df, top_features):
      data = df[top_features].values
      targets_data = df['price_change'].values

      total_samples = len(data) - self.window_size
      if total_samples <= 0:
          raise ValueError("Not enough data for the given window size.")

      # Use NumPy strides to efficiently build rolling windows
      shape = (total_samples, self.window_size, data.shape[1])
      strides = (data.strides[0], data.strides[0], data.strides[1])
      windows = np.lib.stride_tricks.as_strided(data, shape=shape, strides=strides)

      sequences = windows.reshape(total_samples, -1)
      targets = targets_data[self.window_size:]

      return sequences, targets
  </code> </pre>

  <h3>Step 4</h3>
  <p><strong>Feature Scaling</strong></p>
  <pre><code>
  def prepare_data(self, df, top_features):
      X, y = self.create_window_sequences(df, top_features)
      X_scaled = self.scaler.fit_transform(X)
      return X_scaled, y
  </code></pre>

  <h3>Step 5</h3>
  <p><strong>Train-Test Split</strong></p>

  <h3>Step 6</h3>
  <p><strong>Model Training</strong></p>

  <h3>Step 7</h3>
  <p><strong>Time-Series Aware Cross-Validation + Optimization (optimize_model)</strong></p>
  <pre><code>
  def optimize_model(self, X_train, y_train, model_type='xgb', n_iter=20, cv=5):
      def time_series_split(X, n_splits):
          n_samples = len(X)
          k_fold_size = n_samples // n_splits
          indices = np.arange(n_samples)
          for i in range(n_splits):
              test_start = k_fold_size * i
              test_end = k_fold_size * (i + 1) if i < n_splits - 1 else n_samples
              train_indices = indices[:test_start]
              test_indices = indices[test_start:test_end]
              yield train_indices, test_indices

      param_grid = self.param_distributions[model_type]
      base_model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42, tree_method='hist')
      ts_cv = list(time_series_split(X_train, n_splits=cv))

      random_search = RandomizedSearchCV(
          estimator=base_model,
          param_distributions=param_grid,
          n_iter=n_iter,
          cv=ts_cv,
          scoring='neg_mean_squared_error',
          random_state=42,
          n_jobs=-1,
          verbose=2
      )

      random_search.fit(X_train, y_train)
      self.best_params = random_search.best_params_
      self.best_score = random_search.best_score_
      self.models[model_type] = random_search.best_estimator_
      print("\nBest parameters found:")
      print(self.best_params)
      print(f"Best score: {self.best_score:.4f}")
      return self.models[model_type]
  </code></pre>


  </section>


  <section>
    <h2>Long Short-Term Memory (LSTM)</h2>
    <p>LSTM is a recurrent neural network suitable for sequential data:</p>
    <ul>
      <li><strong>Memory Cells:</strong> Preserve long-term dependencies.</li>
      <li><strong>Gates:</strong> Input, forget, and output gates control information flow.</li>
      <li><strong>Training:</strong> Backpropagation Through Time (BPTT) with gradient clipping.</li>
    </ul>
    <p>Used to predict future price movements based on historical windows.</p>
  </section>
  <footer>
    <p>&copy; 2025 QuantDoc</p>
  </footer>
</body>
</html>
