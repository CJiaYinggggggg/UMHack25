<!-- models.html -->
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Models Overview</title>
  <link rel="stylesheet" href="style.css">
</head>
<body>
  <header>
    <h1>Models</h1>
    <nav><a href="index.html">&#8592; Home</a></nav>
  </header>
  
  <section>
    <h2>Hidden Markov Model (HMM)</h2>
    <p>An HMM uses latent (hidden) states to model market regimes. It consists of:</p>
    <ul>
      <li><strong>States:</strong> e.g., Bull, Bear.</li>
      <li><strong>Transition Matrix:</strong> Probabilities of switching between states.</li>
      <li><strong>Emission Probabilities:</strong> Likelihood of observations given a state.</li>
    </ul>
    <p>Viterbi algorithm finds the most probable state sequence given observed returns.</p>
  </section>
  <section>
    <h2>XGBoost</h2>
    <p>XGBoost is a gradient boosting framework that uses decision trees. Key features include:</p>
    <ul>
      <li><strong>Boosting:</strong> Sequentially adds weak learners to minimize loss.</li>
      <li><strong>Regularization:</strong> Controls overfitting with L1 and L2 penalties.</li>
      <li><strong>Parallelization:</strong> Efficiently handles large datasets.</li>
    <h3>Step 1</h3>
    <p>Obtain dataset from different regimes (regimes_0.csv, regimes_1.csv, regimes_2.csv..)</p>
    <h3>Step 2</h3>
    <p>Drop column that are not important (compute using HMM calculation)
    </p>
    <h3>Step 3</h3>
    <p><strong>Creating Rolling Windows (create_window_sequences)</strong>
    </p>
    
    <p>Purpose: Converts time series data into supervised learning format by creating rolling windows of past features and corresponding targets.</p>
    <pre><code>   
    def create_window_sequences(self, df, top_features):
      data = df[top_features].values
      targets_data = df['price_change'].values

      total_samples = len(data) - self.window_size
      if total_samples <= 0:
          raise ValueError("Not enough data for the given window size.")

      # Use NumPy strides to efficiently build rolling windows
      shape = (total_samples, self.window_size, data.shape[1])
      strides = (data.strides[0], data.strides[0], data.strides[1])
      windows = np.lib.stride_tricks.as_strided(data, shape=shape, strides=strides)

      sequences = windows.reshape(total_samples, -1)
      targets = targets_data[self.window_size:]

      return sequences, targets
  </code> </pre>

  <h3>Step 4</h3>
  <p><strong>Feature Scaling</strong></p>
  <pre><code>
  def prepare_data(self, df, top_features):
      X, y = self.create_window_sequences(df, top_features)
      X_scaled = self.scaler.fit_transform(X)
      return X_scaled, y
  </code></pre>

  <h3>Step 5</h3>
  <p><strong>Train-Test Split</strong></p>

  <h3>Step 6</h3>
  <p><strong>Model Training</strong></p>

  <h3>Step 7</h3>
  <p><strong>Time-Series Aware Cross-Validation + Optimization (optimize_model)</strong></p>
  <pre><code>
  def optimize_model(self, X_train, y_train, model_type='xgb', n_iter=20, cv=5):
      def time_series_split(X, n_splits):
          n_samples = len(X)
          k_fold_size = n_samples // n_splits
          indices = np.arange(n_samples)
          for i in range(n_splits):
              test_start = k_fold_size * i
              test_end = k_fold_size * (i + 1) if i < n_splits - 1 else n_samples
              train_indices = indices[:test_start]
              test_indices = indices[test_start:test_end]
              yield train_indices, test_indices

      param_grid = self.param_distributions[model_type]
      base_model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42, tree_method='hist')
      ts_cv = list(time_series_split(X_train, n_splits=cv))

      random_search = RandomizedSearchCV(
          estimator=base_model,
          param_distributions=param_grid,
          n_iter=n_iter,
          cv=ts_cv,
          scoring='neg_mean_squared_error',
          random_state=42,
          n_jobs=-1,
          verbose=2
      )

      random_search.fit(X_train, y_train)
      self.best_params = random_search.best_params_
      self.best_score = random_search.best_score_
      self.models[model_type] = random_search.best_estimator_
      print("\nBest parameters found:")
      print(self.best_params)
      print(f"Best score: {self.best_score:.4f}")
      return self.models[model_type]
  </code></pre>


  </section>


  <section>
    <h2>Long Short-Term Memory (LSTM)</h2>
    <p>LSTM is a recurrent neural network suitable for sequential data:</p>
    <ul>
      <li><strong>Memory Cells:</strong> Preserve long-term dependencies.</li>
      <li><strong>Gates:</strong> Input, forget, and output gates control information flow.</li>
      <li><strong>Training:</strong> Backpropagation Through Time (BPTT) with gradient clipping.</li>
    </ul>
    <p>Used to predict future price movements based on historical windows.</p>
  </section>
  <footer>
    <p>&copy; 2025 QuantDoc</p>
  </footer>
</body>
</html>
