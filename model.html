<!-- models.html -->
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Models Overview</title>
  <link rel="stylesheet" href="style.css">
</head>
<body>
  <header>
    <h1>Models</h1>
    <nav><a href="index.html">&#8592; Home</a></nav>
  </header>
  
  <section>
    <h2>Hidden Markov Model (HMM)</h2>
    <p>An HMM uses latent (hidden) states to model market regimes. It consists of:</p>
    <ul>
      <li><strong>States:</strong> e.g., Bull, Bear.</li>
      <li><strong>Transition Matrix:</strong> Probabilities of switching between states.</li>
      <li><strong>Emission Probabilities:</strong> Likelihood of observations given a state.</li>
    </ul>
    <p>Viterbi algorithm finds the most probable state sequence given observed returns.</p>
  </section>
  <section>
    <h2>XGBoost</h2>
    <p>XGBoost is a gradient boosting framework that uses decision trees. Key features include:</p>
    <ul>
      <li><strong>Boosting:</strong> Sequentially adds weak learners to minimize loss.</li>
      <li><strong>Regularization:</strong> Controls overfitting with L1 and L2 penalties.</li>
      <li><strong>Parallelization:</strong> Efficiently handles large datasets.</li>
    <h3>Step 1</h3>
    <p>Obtain dataset from different regimes (regimes_0.csv, regimes_1.csv, regimes_2.csv..)</p>
    <h3>Step 2</h3>
    <p>Drop column that are not important (compute using HMM calculation)
    </p>
    <h3>Step 3</h3>
    <p><strong>Creating Rolling Windows (create_window_sequences)</strong>
    </p>
    
    <p>Purpose: Converts time series data into supervised learning format by creating rolling windows of past features and corresponding targets.</p>
    <pre><code>   
    def create_window_sequences(self, df, top_features):
      data = df[top_features].values
      targets_data = df['price_change'].values

      total_samples = len(data) - self.window_size
      if total_samples <= 0:
          raise ValueError("Not enough data for the given window size.")

      # Use NumPy strides to efficiently build rolling windows
      shape = (total_samples, self.window_size, data.shape[1])
      strides = (data.strides[0], data.strides[0], data.strides[1])
      windows = np.lib.stride_tricks.as_strided(data, shape=shape, strides=strides)

      sequences = windows.reshape(total_samples, -1)
      targets = targets_data[self.window_size:]

      return sequences, targets
  </code> </pre>

  <h3>Step 4</h3>
  <p>Feature Scaling</p>
  <pre><code>
  def prepare_data(self, df, top_features):
      X, y = self.create_window_sequences(df, top_features)
      X_scaled = self.scaler.fit_transform(X)
      return X_scaled, y
  </code></pre>

  <h3>Step 5</h3>
  <p>Train-Test Split</p>

  <h3>Step 6</h3>
  <p>Model Training</p>

  <h3>Step 7</h3>
  <p>4. Time-Series Aware Cross-Validation + Optimization (optimize_model)</p>
  <pre><code>

  </code></pre>


  </section>


  <section>
    <h2>Long Short-Term Memory (LSTM)</h2>
    <p>LSTM is a recurrent neural network suitable for sequential data:</p>
    <ul>
      <li><strong>Memory Cells:</strong> Preserve long-term dependencies.</li>
      <li><strong>Gates:</strong> Input, forget, and output gates control information flow.</li>
      <li><strong>Training:</strong> Backpropagation Through Time (BPTT) with gradient clipping.</li>
    </ul>
    <p>Used to predict future price movements based on historical windows.</p>
  </section>
  <footer>
    <p>&copy; 2025 QuantDoc</p>
  </footer>
</body>
</html>
